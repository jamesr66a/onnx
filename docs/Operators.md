## Operator Schemas
*This file is automatically generated from the
            [def files](/onnx/defs) via [this script](/onnx/defs/gen_doc.py).
            Do not modify directly and instead edit operator definitions.*

* ai.onnx (default)
  * <a href="#Abs">Abs</a>
  * <a href="#Add">Add</a>
  * <a href="#And">And</a>
  * <a href="#ArgMax">ArgMax</a>
  * <a href="#ArgMin">ArgMin</a>
  * <a href="#AveragePool">AveragePool</a>
  * <a href="#BatchNormalization">BatchNormalization</a>
  * <a href="#Cast">Cast</a>
  * <a href="#Ceil">Ceil</a>
  * <a href="#Clip">Clip</a>
  * <a href="#Concat">Concat</a>
  * <a href="#Constant">Constant</a>
  * <a href="#Conv">Conv</a>
  * <a href="#ConvTranspose">ConvTranspose</a>
  * <a href="#DepthToSpace">DepthToSpace</a>
  * <a href="#Div">Div</a>
  * <a href="#Dropout">Dropout</a>
  * <a href="#Elu">Elu</a>
  * <a href="#Equal">Equal</a>
  * <a href="#Exp">Exp</a>
  * <a href="#Flatten">Flatten</a>
  * <a href="#Floor">Floor</a>
  * <a href="#GRU">GRU</a>
  * <a href="#Gather">Gather</a>
  * <a href="#Gemm">Gemm</a>
  * <a href="#GlobalAveragePool">GlobalAveragePool</a>
  * <a href="#GlobalLpPool">GlobalLpPool</a>
  * <a href="#GlobalMaxPool">GlobalMaxPool</a>
  * <a href="#Greater">Greater</a>
  * <a href="#HardSigmoid">HardSigmoid</a>
  * <a href="#Hardmax">Hardmax</a>
  * <a href="#InstanceNormalization">InstanceNormalization</a>
  * <a href="#LRN">LRN</a>
  * <a href="#LSTM">LSTM</a>
  * <a href="#LeakyRelu">LeakyRelu</a>
  * <a href="#Less">Less</a>
  * <a href="#Log">Log</a>
  * <a href="#LogSoftmax">LogSoftmax</a>
  * <a href="#LpNormalization">LpNormalization</a>
  * <a href="#LpPool">LpPool</a>
  * <a href="#MatMul">MatMul</a>
  * <a href="#Max">Max</a>
  * <a href="#MaxPool">MaxPool</a>
  * <a href="#MaxRoiPool">MaxRoiPool</a>
  * <a href="#Mean">Mean</a>
  * <a href="#Min">Min</a>
  * <a href="#Mul">Mul</a>
  * <a href="#Neg">Neg</a>
  * <a href="#Not">Not</a>
  * <a href="#Or">Or</a>
  * <a href="#PRelu">PRelu</a>
  * <a href="#Pad">Pad</a>
  * <a href="#Pow">Pow</a>
  * <a href="#RNN">RNN</a>
  * <a href="#RandomNormal">RandomNormal</a>
  * <a href="#RandomNormalLike">RandomNormalLike</a>
  * <a href="#RandomUniform">RandomUniform</a>
  * <a href="#RandomUniformLike">RandomUniformLike</a>
  * <a href="#Reciprocal">Reciprocal</a>
  * <a href="#ReduceL1">ReduceL1</a>
  * <a href="#ReduceL2">ReduceL2</a>
  * <a href="#ReduceLogSum">ReduceLogSum</a>
  * <a href="#ReduceLogSumExp">ReduceLogSumExp</a>
  * <a href="#ReduceMax">ReduceMax</a>
  * <a href="#ReduceMean">ReduceMean</a>
  * <a href="#ReduceMin">ReduceMin</a>
  * <a href="#ReduceProd">ReduceProd</a>
  * <a href="#ReduceSum">ReduceSum</a>
  * <a href="#ReduceSumSquare">ReduceSumSquare</a>
  * <a href="#Relu">Relu</a>
  * <a href="#Reshape">Reshape</a>
  * <a href="#Selu">Selu</a>
  * <a href="#Shape">Shape</a>
  * <a href="#Sigmoid">Sigmoid</a>
  * <a href="#Size">Size</a>
  * <a href="#Slice">Slice</a>
  * <a href="#Softmax">Softmax</a>
  * <a href="#Softplus">Softplus</a>
  * <a href="#Softsign">Softsign</a>
  * <a href="#SpaceToDepth">SpaceToDepth</a>
  * <a href="#Split">Split</a>
  * <a href="#Sqrt">Sqrt</a>
  * <a href="#Squeeze">Squeeze</a>
  * <a href="#Sub">Sub</a>
  * <a href="#Sum">Sum</a>
  * <a href="#Tanh">Tanh</a>
  * <a href="#Tile">Tile</a>
  * <a href="#TopK">TopK</a>
  * <a href="#Transpose">Transpose</a>
  * <a href="#Unsqueeze">Unsqueeze</a>
  * <a href="#Xor">Xor</a>
  * <sub>experimental</sub> <a href="#ATen">ATen</a>
  * <sub>experimental</sub> <a href="#Affine">Affine</a>
  * <sub>experimental</sub> <a href="#ConstantFill">ConstantFill</a>
  * <sub>experimental</sub> <a href="#Crop">Crop</a>
  * <sub>experimental</sub> <a href="#FC">FC</a>
  * <sub>experimental</sub> <a href="#GRUUnit">GRUUnit</a>
  * <sub>experimental</sub> <a href="#GivenTensorFill">GivenTensorFill</a>
  * <sub>experimental</sub> <a href="#Identity">Identity</a>
  * <sub>experimental</sub> <a href="#If">If</a>
  * <sub>experimental</sub> <a href="#ImageScaler">ImageScaler</a>
  * <sub>experimental</sub> <a href="#Loop">Loop</a>
  * <sub>experimental</sub> <a href="#LoopIndexTensor">LoopIndexTensor</a>
  * <sub>experimental</sub> <a href="#MeanVarianceNormalization">MeanVarianceNormalization</a>
  * <sub>experimental</sub> <a href="#ParametricSoftplus">ParametricSoftplus</a>
  * <sub>experimental</sub> <a href="#Scale">Scale</a>
  * <sub>experimental</sub> <a href="#ScaledTanh">ScaledTanh</a>
  * <sub>experimental</sub> <a href="#ThresholdedRelu">ThresholdedRelu</a>
  * <sub>experimental</sub> <a href="#Upsample">Upsample</a>

## ai.onnx (default)
### <a name="Abs"></a><a name="abs">**Abs**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>abs</summary>

```python
node = onnx.helper.make_node(
    'Abs',
    inputs=['x'],
    outputs=['y'],
)
x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.abs(x)

expect(node, inputs=[x], outputs=[y],
       name='test_abs')
```

</details>


### <a name="Add"></a><a name="add">**Add**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>broadcast</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>C</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>add</summary>

```python
node = onnx.helper.make_node(
    'Add',
    inputs=['x', 'y'],
    outputs=['sum'],
)

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.random.randn(3, 4, 5).astype(np.float32)
expect(node, inputs=[x, y], outputs=[x + y],
       name='test_add')
```

</details>


<details>
<summary>add_broadcast</summary>

```python
node = onnx.helper.make_node(
    'Add',
    inputs=['x', 'y'],
    outputs=['sum'],
    broadcast=1,
)

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.random.randn(5).astype(np.float32)
expect(node, inputs=[x, y], outputs=[x + y],
       name='test_add_bcast')
```

</details>


### <a name="And"></a><a name="and">**And**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>broadcast</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>C</tt> : T1</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(bool)</dt>
<dd></dd>
<dt><tt>T1</tt> : tensor(bool)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>and</summary>

```python
node = onnx.helper.make_node(
    'And',
    inputs=['x', 'y'],
    outputs=['and'],
)

# 2d
x = (np.random.randn(3, 4) > 0).astype(np.bool)
y = (np.random.randn(3, 4) > 0).astype(np.bool)
z = np.logical_and(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_and2d')

# 3d
x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
y = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
z = np.logical_and(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_and3d')

# 4d
x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
y = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
z = np.logical_and(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_and4d')
```

</details>


<details>
<summary>and_axis</summary>

```python
x = (np.random.randn(5, 5, 5, 5) > 0).astype(np.bool)
y = (np.random.randn(5) > 0).astype(np.bool)

node = onnx.helper.make_node(
    'And',
    inputs=['x', 'y'],
    outputs=['and'],
    broadcast=1,
    axis=0,
)

z = np.logical_and(x, y[:, np.newaxis, np.newaxis, np.newaxis])
expect(node, inputs=[x, y], outputs=[z],
       name='test_and_axis0')

node = onnx.helper.make_node(
    'And',
    inputs=['x', 'y'],
    outputs=['and'],
    broadcast=1,
    axis=1,
)

z = np.logical_and(x, y[:, np.newaxis, np.newaxis])
expect(node, inputs=[x, y], outputs=[z],
       name='test_and_axis1')

node = onnx.helper.make_node(
    'And',
    inputs=['x', 'y'],
    outputs=['and'],
    broadcast=1,
    axis=2,
)

z = np.logical_and(x, y[:, np.newaxis])
expect(node, inputs=[x, y], outputs=[z],
       name='test_and_axis2')

node = onnx.helper.make_node(
    'And',
    inputs=['x', 'y'],
    outputs=['and'],
    broadcast=1,
    axis=3,
)

z = np.logical_and(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_and_axis3')
```

</details>


<details>
<summary>and_broadcast</summary>

```python
node = onnx.helper.make_node(
    'And',
    inputs=['x', 'y'],
    outputs=['and'],
    broadcast=1,
)

#3d vs 1d
x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
y = (np.random.randn(5) > 0).astype(np.bool)
z = np.logical_and(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_or_bcast3v1d')

#3d vs 2d
x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
y = (np.random.randn(4, 5) > 0).astype(np.bool)
z = np.logical_and(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_or_bcast3v2d')

#4d vs 2d
x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
y = (np.random.randn(5, 6) > 0).astype(np.bool)
z = np.logical_and(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_or_bcast4v2d')

#4d vs 3d
x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
y = (np.random.randn(4, 5, 6) > 0).astype(np.bool)
z = np.logical_and(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_or_bcast4v3d')
```

</details>


### <a name="ArgMax"></a><a name="argmax">**ArgMax**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>keepdims</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : tensor(int32)</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="ArgMin"></a><a name="argmin">**ArgMin**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>keepdims</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : tensor(int32)</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="AveragePool"></a><a name="averagepool">**AveragePool**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>auto_pad</tt> : string</dt>
<dd></dd>
<dt><tt>kernel_shape</tt> : list of ints (required)</dt>
<dd></dd>
<dt><tt>pads</tt> : list of ints</dt>
<dd></dd>
<dt><tt>strides</tt> : list of ints</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="BatchNormalization"></a><a name="batchnormalization">**BatchNormalization**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>epsilon</tt> : float</dt>
<dd></dd>
<dt><tt>is_test</tt> : int</dt>
<dd></dd>
<dt><tt>momentum</tt> : float</dt>
<dd></dd>
<dt><tt>spatial</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
<dt><tt>scale</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
<dt><tt>mean</tt> : T</dt>
<dd></dd>
<dt><tt>var</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs (1 - 5)

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
<dt><tt>mean</tt> (optional) : T</dt>
<dd></dd>
<dt><tt>var</tt> (optional) : T</dt>
<dd></dd>
<dt><tt>saved_mean</tt> (optional) : T</dt>
<dd></dd>
<dt><tt>saved_var</tt> (optional) : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="Cast"></a><a name="cast">**Cast**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>to</tt> : string (required)</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T1</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T2</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T1</tt> : tensor(float16), tensor(float), tensor(double), tensor(int8), tensor(int16), tensor(int32), tensor(int64), tensor(uint8), tensor(uint16), tensor(uint32), tensor(uint64), tensor(bool)</dt>
<dd></dd>
<dt><tt>T2</tt> : tensor(float16), tensor(float), tensor(double), tensor(int8), tensor(int16), tensor(int32), tensor(int64), tensor(uint8), tensor(uint16), tensor(uint32), tensor(uint64), tensor(bool)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>cast</summary>

```python
shape = (3, 4)
test_cases = [
    ('FLOAT', 'FLOAT16'),
    ('FLOAT', 'DOUBLE'),
    ('FLOAT16', 'FLOAT'),
    ('FLOAT16', 'DOUBLE'),
    ('DOUBLE', 'FLOAT'),
    ('DOUBLE', 'FLOAT16'),
]   

for case in test_cases:
    from_type = case[0]
    to_type = case[1]
    input = np.random.random_sample(shape).astype(TENSOR_TYPE_TO_NP_TYPE[getattr(TensorProto, from_type)])
    node = onnx.helper.make_node(
        'Cast',
        inputs=['input'],
        outputs=['output'],
        to=to_type
    )
    output = input.astype(TENSOR_TYPE_TO_NP_TYPE[getattr(TensorProto, to_type)])
    expect(node, inputs=[input], outputs=[output], name='test_cast_' + from_type + '_to_' + to_type)
```

</details>


### <a name="Ceil"></a><a name="ceil">**Ceil**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>ceil</summary>

```python
node = onnx.helper.make_node(
    'Ceil',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([-1.5, 1.2]).astype(np.float32)
y = np.ceil(x) #expected output [-1., 2.]
expect(node, inputs=[x], outputs=[y],
       name='test_ceil_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.ceil(x)
expect(node, inputs=[x], outputs=[y],
       name='test_ceil')
```

</details>


### <a name="Clip"></a><a name="clip">**Clip**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>max</tt> : float</dt>
<dd></dd>
<dt><tt>min</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>clip</summary>

```python
node = onnx.helper.make_node(
    'Clip',
    inputs=['x'],
    outputs=['y'],
    min=-1.0,
    max=1.0
)

x = np.array([-2, 0, 2]).astype(np.float32)
y = np.clip(x, -1, 1) #expected output [-1., 0., 1.]
expect(node, inputs=[x], outputs=[y],
       name='test_clip_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x, -1.0, 1.0)
expect(node, inputs=[x], outputs=[y],
       name='test_clip')
```

</details>


<details>
<summary>clip_default</summary>

```python
node = onnx.helper.make_node(
    'Clip',
    inputs=['x'],
    outputs=['y'],
    min=0.0
)
x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x, 0.0, np.inf)
expect(node, inputs=[x], outputs=[y],
       name='test_clip_default_min')

node = onnx.helper.make_node(
    'Clip',
    inputs=['x'],
    outputs=['y'],
    max=0.0
)
x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x, -np.inf, 0.0)
expect(node, inputs=[x], outputs=[y],
       name='test_clip_default_max')
```

</details>


### <a name="Concat"></a><a name="concat">**Concat**</a>

#### Versioning

This operator is used if you are using version 4 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 4
}
~~~~

Other versions of this operator: <a href="Changelog.md#Concat-1">Concat-1</a>

#### Attributes

<dl>
<dt><tt>axis</tt> : int (required)</dt>
<dd></dd>
</dl>

#### Inputs (1 - &#8734;)

<dl>
<dt><tt>inputs</tt> (variadic) : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>concat_result</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>concat</summary>

```python
test_cases = {
    '1d': ([1, 2],
           [3, 4]),
    '2d': ([[1, 2], [3, 4]],
           [[5, 6], [7, 8]]),
    '3d':([[[1, 2], [3, 4]], [[5, 6], [7, 8]]],
           [[[9, 10], [11, 12]], [[13, 14], [15, 16]]])
    }

for test_case, values in test_cases.items():
    values = [np.asarray(v) for v in values]
    for i in range(len(values[0].shape)):
        in_args = ['value' + str(k) for k in range(len(values))]
        node = onnx.helper.make_node(
            'Concat',
            inputs=[s for s in in_args],
            outputs=['output'],
            axis=i
        )
        output = np.concatenate(values,i)
        expect(node, inputs=[v for v in values], outputs=[output],
        name='test_concat_' + test_case + '_axis_' + str(i))
```

</details>


### <a name="Constant"></a><a name="constant">**Constant**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>value</tt> : tensor (required)</dt>
<dd></dd>
</dl>

#### Inputs


#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>constant</summary>

```python
values = np.random.randn(5, 5).astype(np.float32)
node = onnx.helper.make_node(
    'Constant',
    inputs=[],
    outputs=['values'],
    value=onnx.helper.make_tensor(
        name='const_tensor',
        data_type=onnx.TensorProto.FLOAT,
        dims=values.shape,
        vals=values.flatten().astype(float),
    ),
)

expect(node, inputs=[], outputs=[values],
       name='test_constant')
```

</details>


### <a name="Conv"></a><a name="conv">**Conv**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>auto_pad</tt> : string</dt>
<dd></dd>
<dt><tt>dilations</tt> : list of ints</dt>
<dd></dd>
<dt><tt>group</tt> : int</dt>
<dd></dd>
<dt><tt>kernel_shape</tt> : list of ints</dt>
<dd></dd>
<dt><tt>pads</tt> : list of ints</dt>
<dd></dd>
<dt><tt>strides</tt> : list of ints</dt>
<dd></dd>
</dl>

#### Inputs (2 - 3)

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
<dt><tt>W</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> (optional) : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>conv</summary>

```python

x = np.array([[[[  0.,   1.,   2.,   3.,   4.], # (1, 1, 5, 5) input tensor
                [  5.,   6.,   7.,   8.,   9.],
                [ 10.,  11.,  12.,  13.,  14.],
                [ 15.,  16.,  17.,  18.,  19.],
                [ 20.,  21.,  22.,  23.,  24.]]]]).astype(np.float32)
W = np.array([[[[ 1.,  1.,  1.], # (1, 1, 3, 3) tensor for convolution weights
                [ 1.,  1.,  1.],
                [ 1.,  1.,  1.]]]]).astype(np.float32)

# Convolution with padding
node_with_padding = onnx.helper.make_node(
    'Conv',
    inputs=['x', 'W'],
    outputs=['y'],
    kernel_shape=[3, 3],
    pads=[1, 1, 1, 1], # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1
)
y_with_padding = np.array([[[[  12.,   21.,   27.,   33.,   24.], # (1, 1, 5, 5) output tensor
                             [  33.,   54.,   63.,   72.,   51.],
                             [  63.,   99.,  108.,  117.,   81.],
                             [  93.,  144.,  153.,  162.,  111.],
                             [  72.,  111.,  117.,  123.,   84.]]]]).astype(np.float32)
expect(node_with_padding, inputs=[x, W], outputs=[y_with_padding],
   name='test_basic_conv_with_padding')

# Convolution without padding
node_without_padding = onnx.helper.make_node(
    'Conv',
    inputs=['x', 'W'],
    outputs=['y'],
    kernel_shape=[3, 3],
    pads=[0, 0, 0, 0], # Default values for other attributes: strides=[1, 1], dilations=[1, 1], groups=1
)
y_without_padding = np.array([[[[  54.,   63.,   72.], # (1, 1, 3, 3) output tensor
                                [  99.,  108.,  117.],
                                [ 144.,  153.,  162.]]]]).astype(np.float32)
expect(node_without_padding, inputs=[x, W], outputs=[y_without_padding],
   name='test_basic_conv_without_padding')
```

</details>


<details>
<summary>conv_with_strides</summary>

```python

x = np.array([[[[  0.,   1.,   2.,   3.,   4.],  # (1, 1, 7, 5) input tensor
                [  5.,   6.,   7.,   8.,   9.],
                [ 10.,  11.,  12.,  13.,  14.],
                [ 15.,  16.,  17.,  18.,  19.],
                [ 20.,  21.,  22.,  23.,  24.],
                [ 25.,  26.,  27.,  28.,  29.],
                [ 30.,  31.,  32.,  33.,  34.]]]]).astype(np.float32)
W = np.array([[[[ 1.,  1.,  1.],  # (1, 1, 3, 3) tensor for convolution weights
                [ 1.,  1.,  1.],
                [ 1.,  1.,  1.]]]]).astype(np.float32)

# Convolution with strides=2 and padding
node_with_padding = onnx.helper.make_node(
    'Conv',
    inputs=['x', 'W'],
    outputs=['y'],
    kernel_shape=[3, 3],
    pads=[1, 1, 1, 1],
    strides=[2, 2], # Default values for other attributes: dilations=[1, 1], groups=1
)
y_with_padding = np.array([[[[  12.,   27.,   24.], # (1, 1, 4, 3) output tensor
                             [  63.,  108.,   81.],
                             [ 123.,  198.,  141.],
                             [ 112.,  177.,  124.]]]]).astype(np.float32)
expect(node_with_padding, inputs=[x, W], outputs=[y_with_padding],
   name='test_conv_with_strides_padding')

# Convolution with strides=2 and no padding
node_without_padding = onnx.helper.make_node(
    'Conv',
    inputs=['x', 'W'],
    outputs=['y'],
    kernel_shape=[3, 3],
    pads=[0, 0, 0, 0],
    strides=[2, 2], # Default values for other attributes: dilations=[1, 1], groups=1
)
y_without_padding = np.array([[[[  54.,   72.], # (1, 1, 3, 2) output tensor
                                [ 144., 162.],
                                [ 234.,  252.]]]]).astype(np.float32)
expect(node_without_padding, inputs=[x, W], outputs=[y_without_padding],
   name='test_conv_with_strides_no_padding')

# Convolution with strides=2 and padding only along one dimension (the H dimension in NxCxHxW tensor)
node_with_asymmetric_padding = onnx.helper.make_node(
    'Conv',
    inputs=['x', 'W'],
    outputs=['y'],
    kernel_shape=[3, 3],
    pads=[1, 0, 1, 0],
    strides=[2, 2], # Default values for other attributes: dilations=[1, 1], groups=1
)
y_with_asymmetric_padding = np.array([[[[  21.,   33.], # (1, 1, 4, 2) output tensor
                                        [  99.,  117.],
                                        [ 189.,  207.],
                                        [ 171.,  183.]]]]).astype(np.float32)
expect(node_with_asymmetric_padding, inputs=[x, W], outputs=[y_with_asymmetric_padding],
   name='test_conv_with_strides_and_asymmetric_padding')
```

</details>


### <a name="ConvTranspose"></a><a name="convtranspose">**ConvTranspose**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>auto_pad</tt> : string</dt>
<dd></dd>
<dt><tt>dilations</tt> : list of ints</dt>
<dd></dd>
<dt><tt>group</tt> : int</dt>
<dd></dd>
<dt><tt>kernel_shape</tt> : list of ints</dt>
<dd></dd>
<dt><tt>output_padding</tt> : list of ints</dt>
<dd></dd>
<dt><tt>output_shape</tt> : list of ints</dt>
<dd></dd>
<dt><tt>pads</tt> : list of ints</dt>
<dd></dd>
<dt><tt>strides</tt> : list of ints</dt>
<dd></dd>
</dl>

#### Inputs (2 - 3)

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
<dt><tt>W</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> (optional) : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="DepthToSpace"></a><a name="depthtospace">**DepthToSpace**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>blocksize</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="Div"></a><a name="div">**Div**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>broadcast</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>C</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>div</summary>

```python
node = onnx.helper.make_node(
    'Div',
    inputs=['x', 'y'],
    outputs=['z'],
)

x = np.array([3, 4]).astype(np.float32)
y = np.array([1, 2]).astype(np.float32)
z = x / y #expected output [3., 2.]
expect(node, inputs=[x, y], outputs=[z],
       name='test_div_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.random.rand(3, 4, 5).astype(np.float32) + 1.0
z = x / y
expect(node, inputs=[x, y], outputs=[z],
       name='test_div')
```

</details>


<details>
<summary>div_broadcast</summary>

```python
node = onnx.helper.make_node(
    'Div',
    inputs=['x', 'y'],
    outputs=['z'],
    broadcast=1,
)

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.random.rand(5).astype(np.float32) + 1.0
z = x / y
expect(node, inputs=[x, y], outputs=[z],
       name='test_div_bcast')
```

</details>


### <a name="Dropout"></a><a name="dropout">**Dropout**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>is_test</tt> : int</dt>
<dd></dd>
<dt><tt>ratio</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs (1 - 2)

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
<dt><tt>mask</tt> (optional) : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="Elu"></a><a name="elu">**Elu**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>alpha</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>elu</summary>

```python
node = onnx.helper.make_node(
    'Elu',
    inputs=['x'],
    outputs=['y'],
    alpha=2.0
)

x = np.array([-1, 0, 1]).astype(np.float32)
#expected output [-1.2642411, 0., 1.]
y = np.clip(x, 0, np.inf) + (np.exp(np.clip(x, -np.inf, 0)) - 1) * 2.0
expect(node, inputs=[x], outputs=[y],
       name='test_elu_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x, 0, np.inf) + (np.exp(np.clip(x, -np.inf, 0)) - 1) * 2.0
expect(node, inputs=[x], outputs=[y],
       name='test_elu')
```

</details>


<details>
<summary>elu_default</summary>

```python
default_alpha = 1.0
node = onnx.helper.make_node(
    'Elu',
    inputs=['x'],
    outputs=['y'],
)
x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x, 0, np.inf) + (np.exp(np.clip(x, -np.inf, 0)) - 1) * default_alpha
expect(node, inputs=[x], outputs=[y],
       name='test_elu_default')
```

</details>


### <a name="Equal"></a><a name="equal">**Equal**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>broadcast</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>C</tt> : T1</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(bool), tensor(int32), tensor(int64)</dt>
<dd></dd>
<dt><tt>T1</tt> : tensor(bool)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>equal</summary>

```python
node = onnx.helper.make_node(
    'Equal',
    inputs=['x', 'y'],
    outputs=['z'],
)

x = (np.random.randn(3, 4, 5) * 10).astype(np.int32)
y = (np.random.randn(3, 4, 5) * 10).astype(np.int32)
z = np.equal(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_equal')
```

</details>


<details>
<summary>equal_broadcast</summary>

```python
node = onnx.helper.make_node(
    'Equal',
    inputs=['x', 'y'],
    outputs=['z'],
    broadcast=1,
)

x = (np.random.randn(3, 4, 5) * 10).astype(np.int32)
y = (np.random.randn(5) * 10).astype(np.int32)
z = np.equal(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_equal_bcast')
```

</details>


### <a name="Exp"></a><a name="exp">**Exp**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>exp</summary>

```python
node = onnx.helper.make_node(
    'Exp',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([-1, 0, 1]).astype(np.float32)
y = np.exp(x) #expected output [0.36787945, 1., 2.71828175]
expect(node, inputs=[x], outputs=[y],
       name='test_exp_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.exp(x)
expect(node, inputs=[x], outputs=[y],
       name='test_exp')
```

</details>


### <a name="Flatten"></a><a name="flatten">**Flatten**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>flatten</summary>

```python
shape = (2, 3, 4, 5)
a = np.random.random_sample(shape).astype(np.float32)

for i in range(len(shape)):
    node = onnx.helper.make_node(
        'Flatten',
        inputs=['a'],
        outputs=['b'],
        axis=i,
    )

    new_shape = (1, -1) if i == 0 else (np.prod(shape[0:i]).astype(int), -1)
    b= np.reshape(a, new_shape)
    expect(node, inputs=[a], outputs=[b],
       name='test_flatten_axis' + str(i))
```

</details>


<details>
<summary>flatten_with_default_axis</summary>

```python
node = onnx.helper.make_node(
    'Flatten',
    inputs=['a'],
    outputs=['b'], # Default value for axis: axis=1
)

shape = (5, 4, 3, 2)
a = np.random.random_sample(shape).astype(np.float32)
new_shape = (5, 24)
b= np.reshape(a, new_shape)
expect(node, inputs=[a], outputs=[b],
       name='test_flatten_default_axis')
```

</details>


### <a name="Floor"></a><a name="floor">**Floor**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>floor</summary>

```python
node = onnx.helper.make_node(
    'Floor',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([-1.5, 1.2, 2]).astype(np.float32)
y = np.floor(x) #expected output [-2., 1., 2.]
expect(node, inputs=[x], outputs=[y],
       name='test_floor_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.floor(x)
expect(node, inputs=[x], outputs=[y],
       name='test_floor')
```

</details>


### <a name="GRU"></a><a name="gru">**GRU**</a>

#### Versioning

This operator is used if you are using version 3 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 3
}
~~~~

Other versions of this operator: <a href="Changelog.md#GRU-1">GRU-1</a>

#### Attributes

<dl>
<dt><tt>activation_alpha</tt> : list of floats</dt>
<dd></dd>
<dt><tt>activation_beta</tt> : list of floats</dt>
<dd></dd>
<dt><tt>activations</tt> : list of strings</dt>
<dd></dd>
<dt><tt>clip</tt> : float</dt>
<dd></dd>
<dt><tt>direction</tt> : string</dt>
<dd></dd>
<dt><tt>hidden_size</tt> : int</dt>
<dd></dd>
<dt><tt>linear_before_reset</tt> : int</dt>
<dd></dd>
<dt><tt>output_sequence</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs (3 - 6)

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
<dt><tt>W</tt> : T</dt>
<dd></dd>
<dt><tt>R</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> (optional) : T</dt>
<dd></dd>
<dt><tt>sequence_lens</tt> (optional) : T1</dt>
<dd></dd>
<dt><tt>initial_h</tt> (optional) : T</dt>
<dd></dd>
</dl>

#### Outputs (0 - 2)

<dl>
<dt><tt>Y</tt> (optional) : T</dt>
<dd></dd>
<dt><tt>Y_h</tt> (optional) : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
<dt><tt>T1</tt> : tensor(int32)</dt>
<dd></dd>
</dl>


### <a name="Gather"></a><a name="gather">**Gather**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
<dt><tt>indices</tt> : Tind</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
<dt><tt>Tind</tt> : tensor(int32), tensor(int64)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>gather_0</summary>

```python
node = onnx.helper.make_node(
    'Gather',
    inputs=['data', 'indices'],
    outputs=['y'],
    axis=0,
)
data = np.random.randn(5, 4, 3, 2).astype(np.float32)
indices = np.array([0, 1, 3])
y = np.take(data, indices, axis=0)

expect(node, inputs=[data, indices], outputs=[y],
       name='test_gather_0')
```

</details>


<details>
<summary>gather_1</summary>

```python
node = onnx.helper.make_node(
    'Gather',
    inputs=['data', 'indices'],
    outputs=['y'],
    axis=1,
)
data = np.random.randn(5, 4, 3, 2).astype(np.float32)
indices = np.array([0, 1, 3])
y = np.take(data, indices, axis=1)

expect(node, inputs=[data, indices], outputs=[y],
       name='test_gather_1')
```

</details>


### <a name="Gemm"></a><a name="gemm">**Gemm**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>alpha</tt> : float</dt>
<dd></dd>
<dt><tt>beta</tt> : float</dt>
<dd></dd>
<dt><tt>broadcast</tt> : int</dt>
<dd></dd>
<dt><tt>transA</tt> : int</dt>
<dd></dd>
<dt><tt>transB</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
<dt><tt>C</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="GlobalAveragePool"></a><a name="globalaveragepool">**GlobalAveragePool**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>globalaveragepool</summary>

```python
node = onnx.helper.make_node(
    'GlobalAveragePool',
    inputs=['x'],
    outputs=['y'],
)
x = np.random.randn(1, 3, 5, 5).astype(np.float32)
spatial_shape = np.ndim(x) - 2
y = np.average(x, axis=tuple(range(spatial_shape, spatial_shape + 2)))
for _ in range(spatial_shape):
    y = np.expand_dims(y, -1)
expect(node, inputs=[x], outputs=[y], name='test_globalaveragepool')
```

</details>


<details>
<summary>globalaveragepool_precomputed</summary>

```python

node = onnx.helper.make_node(
    'GlobalAveragePool',
    inputs=['x'],
    outputs=['y'],
)
x = np.array([[[
  [1, 2, 3],
  [4, 5, 6],
  [7, 8, 9],
]]]).astype(np.float32)
y = np.array([[[[5]]]]).astype(np.float32)
expect(node, inputs=[x], outputs=[y], name='test_globalaveragepool_precomputed')
```

</details>


### <a name="GlobalLpPool"></a><a name="globallppool">**GlobalLpPool**</a>

#### Versioning

This operator is used if you are using version 2 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 2
}
~~~~

Other versions of this operator: <a href="Changelog.md#GlobalLpPool-1">GlobalLpPool-1</a>

#### Attributes

<dl>
<dt><tt>p</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="GlobalMaxPool"></a><a name="globalmaxpool">**GlobalMaxPool**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>globalmaxpool</summary>

```python

node = onnx.helper.make_node(
    'GlobalMaxPool',
    inputs=['x'],
    outputs=['y'],
)
x = np.random.randn(1, 3, 5, 5).astype(np.float32)
spatial_shape = np.ndim(x) - 2
y = np.max(x, axis=tuple(range(spatial_shape, spatial_shape + 2)))
for _ in range(spatial_shape):
    y = np.expand_dims(y, -1)
expect(node, inputs=[x], outputs=[y], name='test_globalmaxpool')
```

</details>


<details>
<summary>globalmaxpool_precomputed</summary>

```python

node = onnx.helper.make_node(
    'GlobalMaxPool',
    inputs=['x'],
    outputs=['y'],
)
x = np.array([[[
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9],
]]]).astype(np.float32)
y = np.array([[[[9]]]]).astype(np.float32)
expect(node, inputs=[x], outputs=[y], name='test_globalmaxpool_precomputed')
```

</details>


### <a name="Greater"></a><a name="greater">**Greater**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>broadcast</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>C</tt> : T1</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
<dt><tt>T1</tt> : tensor(bool)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>greater</summary>

```python
node = onnx.helper.make_node(
    'Greater',
    inputs=['x', 'y'],
    outputs=['greater'],
)

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.random.randn(3, 4, 5).astype(np.float32)
z = np.greater(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_greater')
```

</details>


<details>
<summary>greater_broadcast</summary>

```python
node = onnx.helper.make_node(
    'Greater',
    inputs=['x', 'y'],
    outputs=['greater'],
    broadcast=1,
)

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.random.randn(5).astype(np.float32)
z = np.greater(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_greater_bcast')
```

</details>


### <a name="HardSigmoid"></a><a name="hardsigmoid">**HardSigmoid**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>alpha</tt> : float</dt>
<dd></dd>
<dt><tt>beta</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>hardsigmoid</summary>

```python
node = onnx.helper.make_node(
    'HardSigmoid',
    inputs=['x'],
    outputs=['y'],
    alpha=0.5,
    beta=0.6
)

x = np.array([-1, 0, 1]).astype(np.float32)
y = np.clip(x * 0.5 + 0.6, 0, 1) #expected output [0.1, 0.6, 1.]
expect(node, inputs=[x], outputs=[y],
       name='test_hardsigmoid_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x * 0.5 + 0.6, 0, 1)
expect(node, inputs=[x], outputs=[y],
       name='test_hardsigmoid')
```

</details>


<details>
<summary>hardsigmoid_default</summary>

```python
default_alpha = 0.2
default_beta = 0.5
node = onnx.helper.make_node(
    'HardSigmoid',
    inputs=['x'],
    outputs=['y'],
)
x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x * default_alpha + default_beta, 0, 1)
expect(node, inputs=[x], outputs=[y],
       name='test_hardsigmoid_default')
```

</details>


### <a name="Hardmax"></a><a name="hardmax">**Hardmax**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>hardmax</summary>

```python
node = onnx.helper.make_node(
    'Hardmax',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([[3, 0, 1, 2], [2, 5, 1, 0], [0, 1, 3, 2], [0, 1, 2, 3]]).astype(np.float32)
y = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]).astype(np.float32)
expect(node, inputs=[x], outputs=[y],
       name='test_hardmax_example')

# For multiple occurrances of the maximal values, the first occurrence is selected for one-hot output
x = np.array([[3, 3, 3, 1]]).astype(np.float32)
y = np.array([[1, 0, 0, 0]]).astype(np.float32)
expect(node, inputs=[x], outputs=[y],
       name='test_hardmax_one_hot')
```

</details>


<details>
<summary>hardmax_axis</summary>

```python
def hardmax_2d(x):
    return np.eye(x.shape[1])[np.argmax(x,axis=1)]

x = np.random.randn(3, 4, 5).astype(np.float32)
node = onnx.helper.make_node(
    'Hardmax',
    inputs=['x'],
    outputs=['y'],
    axis=0,
)
y = hardmax_2d(x.reshape(1, 60)).reshape(3, 4, 5)
expect(node, inputs=[x], outputs=[y],
       name='test_hardmax_axis_0')

node = onnx.helper.make_node(
    'Hardmax',
    inputs=['x'],
    outputs=['y'],
    axis=1,
)
y = hardmax_2d(x.reshape(3, 20)).reshape(3, 4, 5)
expect(node, inputs=[x], outputs=[y],
       name='test_hardmax_axis_1')

# default axis is 1
node = onnx.helper.make_node(
    'Hardmax',
    inputs=['x'],
    outputs=['y'],
)
expect(node, inputs=[x], outputs=[y],
       name='test_hardmax_default_axis')

node = onnx.helper.make_node(
    'Hardmax',
    inputs=['x'],
    outputs=['y'],
    axis=2,
)
y = hardmax_2d(x.reshape(12, 5)).reshape(3, 4, 5)
expect(node, inputs=[x], outputs=[y],
       name='test_hardmax_axis_2')
```

</details>


### <a name="InstanceNormalization"></a><a name="instancenormalization">**InstanceNormalization**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>epsilon</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
<dt><tt>scale</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="LRN"></a><a name="lrn">**LRN**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>alpha</tt> : float (required)</dt>
<dd></dd>
<dt><tt>beta</tt> : float (required)</dt>
<dd></dd>
<dt><tt>bias</tt> : float</dt>
<dd></dd>
<dt><tt>size</tt> : int (required)</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="LSTM"></a><a name="lstm">**LSTM**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>activation_alpha</tt> : list of floats</dt>
<dd></dd>
<dt><tt>activation_beta</tt> : list of floats</dt>
<dd></dd>
<dt><tt>activations</tt> : list of strings</dt>
<dd></dd>
<dt><tt>clip</tt> : float</dt>
<dd></dd>
<dt><tt>direction</tt> : string</dt>
<dd></dd>
<dt><tt>hidden_size</tt> : int</dt>
<dd></dd>
<dt><tt>input_forget</tt> : int</dt>
<dd></dd>
<dt><tt>output_sequence</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs (3 - 8)

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
<dt><tt>W</tt> : T</dt>
<dd></dd>
<dt><tt>R</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> (optional) : T</dt>
<dd></dd>
<dt><tt>sequence_lens</tt> (optional) : T1</dt>
<dd></dd>
<dt><tt>initial_h</tt> (optional) : T</dt>
<dd></dd>
<dt><tt>initial_c</tt> (optional) : T</dt>
<dd></dd>
<dt><tt>P</tt> (optional) : T</dt>
<dd></dd>
</dl>

#### Outputs (0 - 3)

<dl>
<dt><tt>Y</tt> (optional) : T</dt>
<dd></dd>
<dt><tt>Y_h</tt> (optional) : T</dt>
<dd></dd>
<dt><tt>Y_c</tt> (optional) : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
<dt><tt>T1</tt> : tensor(int32)</dt>
<dd></dd>
</dl>


### <a name="LeakyRelu"></a><a name="leakyrelu">**LeakyRelu**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>alpha</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>leakyrelu</summary>

```python
node = onnx.helper.make_node(
    'LeakyRelu',
    inputs=['x'],
    outputs=['y'],
    alpha=0.1
)

x = np.array([-1, 0, 1]).astype(np.float32)
#expected output [-0.1, 0., 1.]
y = np.clip(x, 0, np.inf) + np.clip(x, -np.inf, 0) * 0.1
expect(node, inputs=[x], outputs=[y],
       name='test_leakyrelu_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x, 0, np.inf) + np.clip(x, -np.inf, 0) * 0.1
expect(node, inputs=[x], outputs=[y],
       name='test_leakyrelu')
```

</details>


<details>
<summary>leakyrelu_default</summary>

```python
default_alpha = 0.01
node = onnx.helper.make_node(
    'LeakyRelu',
    inputs=['x'],
    outputs=['y'],
)
x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x, 0, np.inf) + np.clip(x, -np.inf, 0) * default_alpha
expect(node, inputs=[x], outputs=[y],
       name='test_leakyrelu_default')
```

</details>


### <a name="Less"></a><a name="less">**Less**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>broadcast</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>C</tt> : T1</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
<dt><tt>T1</tt> : tensor(bool)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>less</summary>

```python
node = onnx.helper.make_node(
    'Less',
    inputs=['x', 'y'],
    outputs=['less'],
)

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.random.randn(3, 4, 5).astype(np.float32)
z = np.less(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_less')
```

</details>


<details>
<summary>less_broadcast</summary>

```python
node = onnx.helper.make_node(
    'Less',
    inputs=['x', 'y'],
    outputs=['less'],
    broadcast=1,
)

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.random.randn(5).astype(np.float32)
z = np.less(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_less_bcast')
```

</details>


### <a name="Log"></a><a name="log">**Log**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>log</summary>

```python
node = onnx.helper.make_node(
    'Log',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([1, 10]).astype(np.float32)
y = np.log(x) #expected output [0., 2.30258512]
expect(node, inputs=[x], outputs=[y],
       name='test_log_example')

x = np.exp(np.random.randn(3, 4, 5).astype(np.float32))
y = np.log(x)
expect(node, inputs=[x], outputs=[y],
       name='test_log')
```

</details>


### <a name="LogSoftmax"></a><a name="logsoftmax">**LogSoftmax**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>logsoftmax</summary>

```python
node = onnx.helper.make_node(
    'LogSoftmax',
    inputs=['x'],
    outputs=['y'],
)
x = np.array([[-1, 0, 1]]).astype(np.float32)
y = x - np.log(np.sum(np.exp(x), axis=1)) #expected output [[-2.40760589, -1.40760589, -0.40760589]]
expect(node, inputs=[x], outputs=[y],
       name='test_logsoftmax_example_1')
```

</details>


<details>
<summary>logsoftmax_axis</summary>

```python
def logsoftmax_2d(x):
    max_x = np.max(x, axis=1).reshape((-1, 1))
    exp_x = np.exp(x - max_x)
    return x - max_x - np.log(np.sum(exp_x, axis=1).reshape((-1, 1)))

x = np.array([[0, 1, 2, 3], [10000, 10001, 10002, 10003]]).astype(np.float32)
#expected output [[-3.4401896, -2.4401896, -1.44018972, -0.44018969],
#                 [-3.4401896, -2.4401896, -1.44018972, -0.44018969]]
y = logsoftmax_2d(x)

node = onnx.helper.make_node(
    'LogSoftmax',
    inputs=['x'],
    outputs=['y'],
)
expect(node, inputs=[x], outputs=[y],
       name='test_logsoftmax_large_number')

x = np.abs(np.random.randn(3, 4, 5).astype(np.float32))
node = onnx.helper.make_node(
    'LogSoftmax',
    inputs=['x'],
    outputs=['y'],
    axis=0,
)
y = logsoftmax_2d(x.reshape(1, 60)).reshape(3, 4, 5)
expect(node, inputs=[x], outputs=[y],
       name='test_logsoftmax_axis_0')

node = onnx.helper.make_node(
    'LogSoftmax',
    inputs=['x'],
    outputs=['y'],
    axis=1,
)
y = logsoftmax_2d(x.reshape(3, 20)).reshape(3, 4, 5)
expect(node, inputs=[x], outputs=[y],
       name='test_logsoftmax_axis_1')

# default axis is 1
node = onnx.helper.make_node(
    'LogSoftmax',
    inputs=['x'],
    outputs=['y'],
)
expect(node, inputs=[x], outputs=[y],
       name='test_logsoftmax_default_axis')

node = onnx.helper.make_node(
    'LogSoftmax',
    inputs=['x'],
    outputs=['y'],
    axis=2,
)
y = logsoftmax_2d(x.reshape(12, 5)).reshape(3, 4, 5)
expect(node, inputs=[x], outputs=[y],
       name='test_logsoftmax_axis_2')
```

</details>


### <a name="LpNormalization"></a><a name="lpnormalization">**LpNormalization**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>p</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="LpPool"></a><a name="lppool">**LpPool**</a>

#### Versioning

This operator is used if you are using version 2 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 2
}
~~~~

Other versions of this operator: <a href="Changelog.md#LpPool-1">LpPool-1</a>

#### Attributes

<dl>
<dt><tt>auto_pad</tt> : string</dt>
<dd></dd>
<dt><tt>kernel_shape</tt> : list of ints (required)</dt>
<dd></dd>
<dt><tt>p</tt> : int</dt>
<dd></dd>
<dt><tt>pads</tt> : list of ints</dt>
<dd></dd>
<dt><tt>strides</tt> : list of ints</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="MatMul"></a><a name="matmul">**MatMul**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>matmul</summary>

```python
node = onnx.helper.make_node(
    'MatMul',
    inputs=['a', 'b'],
    outputs=['c'],
)

# 2d
a = np.random.randn(3, 4).astype(np.float32)
b = np.random.randn(4, 3).astype(np.float32)
c = np.matmul(a, b)
expect(node, inputs=[a, b], outputs=[c],
       name='test_matmul_2d')

# 3d
a = np.random.randn(2, 3, 4).astype(np.float32)
b = np.random.randn(2, 4, 3).astype(np.float32)
c = np.matmul(a, b)
expect(node, inputs=[a, b], outputs=[c],
       name='test_matmul_3d')

# 4d
a = np.random.randn(1, 2, 3, 4).astype(np.float32)
b = np.random.randn(1, 2, 4, 3).astype(np.float32)
c = np.matmul(a, b)
expect(node, inputs=[a, b], outputs=[c],
       name='test_matmul_4d')
```

</details>


### <a name="Max"></a><a name="max">**Max**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs (1 - &#8734;)

<dl>
<dt><tt>data_0</tt> (variadic) : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>max</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>max</summary>

```python
data_0 = np.array([3, 2, 1]).astype(np.float32)
data_1 = np.array([1, 4, 4]).astype(np.float32)
data_2 = np.array([2, 5, 3]).astype(np.float32)
result = np.array([3, 5, 4]).astype(np.float32)
node = onnx.helper.make_node(
    'Max',
    inputs=['data_0', 'data_1', 'data_2'],
    outputs=['result'],
)
expect(node, inputs=[data_0, data_1, data_2], outputs=[result],
       name='test_max_example')

node = onnx.helper.make_node(
    'Max',
    inputs=['data_0'],
    outputs=['result'],
)
expect(node, inputs=[data_0], outputs=[data_0],
       name='test_max_one_input')

result = np.maximum(data_0, data_1)
node = onnx.helper.make_node(
    'Max',
    inputs=['data_0', 'data_1'],
    outputs=['result'],
)
expect(node, inputs=[data_0, data_1], outputs=[result],
       name='test_max_two_inputs')
```

</details>


### <a name="MaxPool"></a><a name="maxpool">**MaxPool**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>auto_pad</tt> : string</dt>
<dd></dd>
<dt><tt>kernel_shape</tt> : list of ints (required)</dt>
<dd></dd>
<dt><tt>pads</tt> : list of ints</dt>
<dd></dd>
<dt><tt>strides</tt> : list of ints</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="MaxRoiPool"></a><a name="maxroipool">**MaxRoiPool**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>pooled_shape</tt> : list of ints (required)</dt>
<dd></dd>
<dt><tt>spatial_scale</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
<dt><tt>rois</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="Mean"></a><a name="mean">**Mean**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs (1 - &#8734;)

<dl>
<dt><tt>data_0</tt> (variadic) : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>mean</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>mean</summary>

```python
data_0 = np.array([3, 0, 2]).astype(np.float32)
data_1 = np.array([1, 3, 4]).astype(np.float32)
data_2 = np.array([2, 6, 6]).astype(np.float32)
result = np.array([2, 3, 4]).astype(np.float32)
node = onnx.helper.make_node(
    'Mean',
    inputs=['data_0', 'data_1', 'data_2'],
    outputs=['result'],
)
expect(node, inputs=[data_0, data_1, data_2], outputs=[result],
       name='test_mean_example')

node = onnx.helper.make_node(
    'Mean',
    inputs=['data_0'],
    outputs=['result'],
)
expect(node, inputs=[data_0], outputs=[data_0],
       name='test_mean_one_input')

result = np.divide(np.add(data_0, data_1), 2.)
node = onnx.helper.make_node(
    'Mean',
    inputs=['data_0', 'data_1'],
    outputs=['result'],
)
expect(node, inputs=[data_0, data_1], outputs=[result],
       name='test_mean_two_inputs')
```

</details>


### <a name="Min"></a><a name="min">**Min**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs (1 - &#8734;)

<dl>
<dt><tt>data_0</tt> (variadic) : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>min</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>min</summary>

```python
data_0 = np.array([3, 2, 1]).astype(np.float32)
data_1 = np.array([1, 4, 4]).astype(np.float32)
data_2 = np.array([2, 5, 0]).astype(np.float32)
result = np.array([1, 2, 0]).astype(np.float32)
node = onnx.helper.make_node(
    'Min',
    inputs=['data_0', 'data_1', 'data_2'],
    outputs=['result'],
)
expect(node, inputs=[data_0, data_1, data_2], outputs=[result],
       name='test_min_example')

node = onnx.helper.make_node(
    'Min',
    inputs=['data_0'],
    outputs=['result'],
)
expect(node, inputs=[data_0], outputs=[data_0],
       name='test_min_one_input')

result = np.minimum(data_0, data_1)
node = onnx.helper.make_node(
    'Min',
    inputs=['data_0', 'data_1'],
    outputs=['result'],
)
expect(node, inputs=[data_0, data_1], outputs=[result],
       name='test_min_two_inputs')
```

</details>


### <a name="Mul"></a><a name="mul">**Mul**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>broadcast</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>C</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>mul</summary>

```python
node = onnx.helper.make_node(
    'Mul',
    inputs=['x', 'y'],
    outputs=['z'],
)

x = np.array([1, 2, 3]).astype(np.float32)
y = np.array([4, 5, 6]).astype(np.float32)
z = x * y #expected output [4., 10., 18.]
expect(node, inputs=[x, y], outputs=[z],
       name='test_mul_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.random.randn(3, 4, 5).astype(np.float32)
z = x * y
expect(node, inputs=[x, y], outputs=[z],
       name='test_mul')
```

</details>


<details>
<summary>mul_broadcast</summary>

```python
node = onnx.helper.make_node(
    'Mul',
    inputs=['x', 'y'],
    outputs=['z'],
    broadcast=1,
)

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.random.randn(5).astype(np.float32)
z = x * y
expect(node, inputs=[x, y], outputs=[z],
       name='test_mul_bcast')
```

</details>


### <a name="Neg"></a><a name="neg">**Neg**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>neg</summary>

```python
node = onnx.helper.make_node(
    'Neg',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([-4, 2]).astype(np.float32)
y = np.negative(x) #expected output [4., -2.],
expect(node, inputs=[x], outputs=[y],
       name='test_neg_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.negative(x)
expect(node, inputs=[x], outputs=[y],
       name='test_neg')
```

</details>


### <a name="Not"></a><a name="not">**Not**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(bool)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>not</summary>

```python
node = onnx.helper.make_node(
    'Not',
    inputs=['x'],
    outputs=['not'],
)

# 2d
x = (np.random.randn(3, 4) > 0).astype(np.bool)
expect(node, inputs=[x], outputs=[np.logical_not(x)],
       name='test_not_2d')

# 3d
x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
expect(node, inputs=[x], outputs=[np.logical_not(x)],
       name='test_not_3d')

# 4d
x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
expect(node, inputs=[x], outputs=[np.logical_not(x)],
       name='test_not_4d')
```

</details>


### <a name="Or"></a><a name="or">**Or**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>broadcast</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>C</tt> : T1</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(bool)</dt>
<dd></dd>
<dt><tt>T1</tt> : tensor(bool)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>or</summary>

```python
node = onnx.helper.make_node(
    'Or',
    inputs=['x', 'y'],
    outputs=['or'],
)

# 2d
x = (np.random.randn(3, 4) > 0).astype(np.bool)
y = (np.random.randn(3, 4) > 0).astype(np.bool)
z = np.logical_or(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_or2d')

# 3d
x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
y = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
z = np.logical_or(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_or3d')

# 4d
x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
y = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
z = np.logical_or(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_or4d')
```

</details>


<details>
<summary>or_axis</summary>

```python
x = (np.random.randn(5, 5, 5, 5) > 0).astype(np.bool)
y = (np.random.randn(5) > 0).astype(np.bool)

node = onnx.helper.make_node(
    'Or',
    inputs=['x', 'y'],
    outputs=['or'],
    broadcast=1,
    axis=0,
)

z = np.logical_or(x, y[:, np.newaxis, np.newaxis, np.newaxis])
expect(node, inputs=[x, y], outputs=[z],
       name='test_or_axis0')

node = onnx.helper.make_node(
    'Or',
    inputs=['x', 'y'],
    outputs=['or'],
    broadcast=1,
    axis=1,
)

z = np.logical_or(x, y[:, np.newaxis, np.newaxis])
expect(node, inputs=[x, y], outputs=[z],
       name='test_or_axis1')

node = onnx.helper.make_node(
    'Or',
    inputs=['x', 'y'],
    outputs=['or'],
    broadcast=1,
    axis=2,
)

z = np.logical_or(x, y[:, np.newaxis])
expect(node, inputs=[x, y], outputs=[z],
       name='test_or_axis2')

node = onnx.helper.make_node(
    'Or',
    inputs=['x', 'y'],
    outputs=['or'],
    broadcast=1,
    axis=3,
)

z = np.logical_or(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_or_axis3')
```

</details>


<details>
<summary>or_broadcast</summary>

```python
node = onnx.helper.make_node(
    'Or',
    inputs=['x', 'y'],
    outputs=['or'],
    broadcast=1,
)

#3d vs 1d
x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
y = (np.random.randn(5) > 0).astype(np.bool)
z = np.logical_or(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_or_bcast3v1d')

#3d vs 2d
x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
y = (np.random.randn(4, 5) > 0).astype(np.bool)
z = np.logical_or(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_or_bcast3v2d')

#4d vs 2d
x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
y = (np.random.randn(5, 6) > 0).astype(np.bool)
z = np.logical_or(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_or_bcast4v2d')

#4d vs 3d
x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
y = (np.random.randn(4, 5, 6) > 0).astype(np.bool)
z = np.logical_or(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_or_bcast4v3d')
```

</details>


### <a name="PRelu"></a><a name="prelu">**PRelu**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
<dt><tt>slope</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="Pad"></a><a name="pad">**Pad**</a>

#### Versioning

This operator is used if you are using version 2 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 2
}
~~~~

Other versions of this operator: <a href="Changelog.md#Pad-1">Pad-1</a>

#### Attributes

<dl>
<dt><tt>mode</tt> : string</dt>
<dd></dd>
<dt><tt>pads</tt> : list of ints (required)</dt>
<dd></dd>
<dt><tt>value</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>constant_pad</summary>

```python
node = onnx.helper.make_node(
    'Pad',
    inputs=['x'],
    outputs=['y'],
    mode='constant',
    value=1.2,
    pads=[0, 0, 1, 3, 0, 0, 2, 4],
)
x = np.random.randn(1, 3, 4, 5).astype(np.float32)
y = np.pad(
    x,
    pad_width=((0, 0), (0, 0), (1, 2), (3, 4)),
    mode='constant',
    constant_values=1.2,
)

expect(node, inputs=[x], outputs=[y],
       name='test_constant_pad')
```

</details>


<details>
<summary>reflection_and_edge_pad</summary>

```python
for mode in ['edge', 'reflect']:
    node = onnx.helper.make_node(
        'Pad',
        inputs=['x'],
        outputs=['y'],
        mode=mode,
        pads=[0, 0, 1, 1, 0, 0, 1, 1]
    )
    x = np.random.randn(1, 3, 4, 5).astype(np.float32)
    y = np.pad(
        x,
        pad_width=((0, 0), (0, 0), (1, 1), (1, 1)),
        mode=mode,
    )

    expect(node, inputs=[x], outputs=[y],
           name='test_{}_pad'.format(mode))
```

</details>


### <a name="Pow"></a><a name="pow">**Pow**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>broadcast</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Z</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>pow</summary>

```python
node = onnx.helper.make_node(
    'Pow',
    inputs=['x', 'y'],
    outputs=['z'],
)

x = np.array([1, 2, 3]).astype(np.float32)
y = np.array([4, 5, 6]).astype(np.float32)
z = np.power(x, y) # expected output [1., 32., 729.]
expect(node, inputs=[x, y], outputs=[z],
       name='test_pow_example')

x = np.arange(60).reshape(3, 4, 5).astype(np.float32)
y = np.random.randn(3, 4, 5).astype(np.float32)
z = np.power(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_pow')
```

</details>


<details>
<summary>pow_broadcast</summary>

```python
node = onnx.helper.make_node(
    'Pow',
    inputs=['x', 'y'],
    outputs=['z'],
    broadcast=1,
)

x = np.array([1, 2, 3]).astype(np.float32)
y = np.array([2]).astype(np.float32)
z = np.power(x, y) # expected output [1., 4., 9.]
expect(node, inputs=[x, y], outputs=[z],
       name='test_pow_bcast')

node = onnx.helper.make_node(
    'Pow',
    inputs=['x', 'y'],
    outputs=['z'],
    broadcast=1,
    axis=0,
)
x = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)
y = np.array([2, 3]).astype(np.float32)
z = np.array([[1, 4, 9], [64, 125, 216]]).astype(np.float32)
expect(node, inputs=[x, y], outputs=[z],
       name='test_pow_bcast_axis0')
```

</details>


### <a name="RNN"></a><a name="rnn">**RNN**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>activation_alpha</tt> : list of floats</dt>
<dd></dd>
<dt><tt>activation_beta</tt> : list of floats</dt>
<dd></dd>
<dt><tt>activations</tt> : list of strings</dt>
<dd></dd>
<dt><tt>clip</tt> : float</dt>
<dd></dd>
<dt><tt>direction</tt> : string</dt>
<dd></dd>
<dt><tt>hidden_size</tt> : int</dt>
<dd></dd>
<dt><tt>output_sequence</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs (3 - 6)

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
<dt><tt>W</tt> : T</dt>
<dd></dd>
<dt><tt>R</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> (optional) : T</dt>
<dd></dd>
<dt><tt>sequence_lens</tt> (optional) : T1</dt>
<dd></dd>
<dt><tt>initial_h</tt> (optional) : T</dt>
<dd></dd>
</dl>

#### Outputs (0 - 2)

<dl>
<dt><tt>Y</tt> (optional) : T</dt>
<dd></dd>
<dt><tt>Y_h</tt> (optional) : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
<dt><tt>T1</tt> : tensor(int32)</dt>
<dd></dd>
</dl>


### <a name="RandomNormal"></a><a name="randomnormal">**RandomNormal**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>dtype</tt> : int</dt>
<dd></dd>
<dt><tt>mean</tt> : float</dt>
<dd></dd>
<dt><tt>scale</tt> : float</dt>
<dd></dd>
<dt><tt>seed</tt> : float</dt>
<dd></dd>
<dt><tt>shape</tt> : list of ints (required)</dt>
<dd></dd>
</dl>

#### Inputs


#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="RandomNormalLike"></a><a name="randomnormallike">**RandomNormalLike**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>dtype</tt> : int</dt>
<dd></dd>
<dt><tt>mean</tt> : float</dt>
<dd></dd>
<dt><tt>scale</tt> : float</dt>
<dd></dd>
<dt><tt>seed</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : tensor(int32)</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="RandomUniform"></a><a name="randomuniform">**RandomUniform**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>dtype</tt> : int</dt>
<dd></dd>
<dt><tt>high</tt> : float</dt>
<dd></dd>
<dt><tt>low</tt> : float</dt>
<dd></dd>
<dt><tt>seed</tt> : float</dt>
<dd></dd>
<dt><tt>shape</tt> : list of ints (required)</dt>
<dd></dd>
</dl>

#### Inputs


#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="RandomUniformLike"></a><a name="randomuniformlike">**RandomUniformLike**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>dtype</tt> : int</dt>
<dd></dd>
<dt><tt>high</tt> : float</dt>
<dd></dd>
<dt><tt>low</tt> : float</dt>
<dd></dd>
<dt><tt>seed</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : tensor(int32)</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="Reciprocal"></a><a name="reciprocal">**Reciprocal**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>reciprocal</summary>

```python
node = onnx.helper.make_node(
    'Reciprocal',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([-4, 2]).astype(np.float32)
y = np.reciprocal(x) #expected output [-0.25, 0.5],
expect(node, inputs=[x], outputs=[y],
       name='test_reciprocal_example')

x = np.random.rand(3, 4, 5).astype(np.float32) + 0.5
y = np.reciprocal(x)
expect(node, inputs=[x], outputs=[y],
       name='test_reciprocal')
```

</details>


### <a name="ReduceL1"></a><a name="reducel1">**ReduceL1**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints</dt>
<dd></dd>
<dt><tt>keepdims</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="ReduceL2"></a><a name="reducel2">**ReduceL2**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints</dt>
<dd></dd>
<dt><tt>keepdims</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="ReduceLogSum"></a><a name="reducelogsum">**ReduceLogSum**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints</dt>
<dd></dd>
<dt><tt>keepdims</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="ReduceLogSumExp"></a><a name="reducelogsumexp">**ReduceLogSumExp**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints</dt>
<dd></dd>
<dt><tt>keepdims</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="ReduceMax"></a><a name="reducemax">**ReduceMax**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints</dt>
<dd></dd>
<dt><tt>keepdims</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="ReduceMean"></a><a name="reducemean">**ReduceMean**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints</dt>
<dd></dd>
<dt><tt>keepdims</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="ReduceMin"></a><a name="reducemin">**ReduceMin**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints</dt>
<dd></dd>
<dt><tt>keepdims</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="ReduceProd"></a><a name="reduceprod">**ReduceProd**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints</dt>
<dd></dd>
<dt><tt>keepdims</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="ReduceSum"></a><a name="reducesum">**ReduceSum**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints</dt>
<dd></dd>
<dt><tt>keepdims</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="ReduceSumSquare"></a><a name="reducesumsquare">**ReduceSumSquare**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints</dt>
<dd></dd>
<dt><tt>keepdims</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reduced</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="Relu"></a><a name="relu">**Relu**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>relu</summary>

```python
node = onnx.helper.make_node(
    'Relu',
    inputs=['x'],
    outputs=['y'],
)
x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x, 0, np.inf)

expect(node, inputs=[x], outputs=[y],
       name='test_relu')
```

</details>


### <a name="Reshape"></a><a name="reshape">**Reshape**</a>

#### Versioning

This operator is used if you are using version 5 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 5
}
~~~~

Other versions of this operator: <a href="Changelog.md#Reshape-1">Reshape-1</a>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
<dt><tt>shape</tt> : tensor(int64)</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>reshaped</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>reshape</summary>

```python
original_shape = [2, 3, 4]
test_cases = {
    'reordered_dims': np.array([4, 2, 3], dtype=np.int64),
    'reduced_dims': np.array([3, 8], dtype=np.int64),
    'extended_dims': np.array([3, 2, 2, 2], dtype=np.int64),
    'one_dim': np.array([24], dtype=np.int64),
    'negative_dim': np.array([6, -1, 2], dtype=np.int64),
}
data = np.random.random_sample(original_shape).astype(np.float32)

for test_name, shape in test_cases.items():
    node = onnx.helper.make_node(
        'Reshape',
        inputs=['data', 'shape'],
        outputs=['reshaped'],
    )

    reshaped = np.reshape(data, shape)
    expect(node, inputs=[data, shape], outputs=[reshaped],
       name='test_reshape_' + test_name)
```

</details>


### <a name="Selu"></a><a name="selu">**Selu**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>alpha</tt> : float</dt>
<dd></dd>
<dt><tt>gamma</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>selu</summary>

```python
node = onnx.helper.make_node(
    'Selu',
    inputs=['x'],
    outputs=['y'],
    alpha=2.0,
    gamma=3.0
)

x = np.array([-1, 0, 1]).astype(np.float32)
#expected output [-3.79272318, 0., 3.]
y = np.clip(x, 0, np.inf) * 3.0 + (np.exp(np.clip(x, -np.inf, 0)) - 1) * 2.0 * 3.0
expect(node, inputs=[x], outputs=[y],
       name='test_selu_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x, 0, np.inf) * 3.0 + (np.exp(np.clip(x, -np.inf, 0)) - 1) * 2.0 * 3.0
expect(node, inputs=[x], outputs=[y],
       name='test_selu')
```

</details>


<details>
<summary>selu_default</summary>

```python
default_alpha = 1.6732
default_gamma = 1.0507
node = onnx.helper.make_node(
    'Selu',
    inputs=['x'],
    outputs=['y'],
)
x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x, 0, np.inf) * default_gamma + (np.exp(np.clip(x, -np.inf, 0)) - 1) * default_alpha * default_gamma
expect(node, inputs=[x], outputs=[y],
       name='test_selu_default')
```

</details>


### <a name="Shape"></a><a name="shape">**Shape**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>shape</tt> : T1</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double), tensor(int8), tensor(int16), tensor(int32), tensor(int64), tensor(uint8), tensor(uint16), tensor(bool)</dt>
<dd></dd>
<dt><tt>T1</tt> : tensor(int64)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>shape</summary>

```python
node = onnx.helper.make_node(
    'Shape',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([
    [1, 2, 3],
    [4, 5, 6],
]).astype(np.float32)
y = np.array([
    2, 3,
]).astype(np.int64)

expect(node, inputs=[x], outputs=[y],
       name='test_shape_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.array(x.shape).astype(np.int64)

expect(node, inputs=[x], outputs=[y],
       name='test_shape')
```

</details>


### <a name="Sigmoid"></a><a name="sigmoid">**Sigmoid**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>sigmoid</summary>

```python
node = onnx.helper.make_node(
    'Sigmoid',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([-1, 0, 1]).astype(np.float32)
y = 1.0 / (1.0 + np.exp(np.negative(x))) #expected output [0.26894143, 0.5, 0.7310586]
expect(node, inputs=[x], outputs=[y],
       name='test_sigmoid_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = 1.0 / (1.0 + np.exp(np.negative(x)))
expect(node, inputs=[x], outputs=[y],
       name='test_sigmoid')
```

</details>


### <a name="Size"></a><a name="size">**Size**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>size</tt> : T1</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double), tensor(int8), tensor(int16), tensor(int32), tensor(int64), tensor(uint8), tensor(uint16), tensor(bool)</dt>
<dd></dd>
<dt><tt>T1</tt> : int64</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>size</summary>

```python
node = onnx.helper.make_node(
    'Size',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([
    [1, 2, 3],
    [4, 5, 6],
]).astype(np.float32)
y = np.array(6).astype(np.int64)

expect(node, inputs=[x], outputs=[y],
       name='test_size_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.array(x.size).astype(np.int64)

expect(node, inputs=[x], outputs=[y],
       name='test_size')
```

</details>


### <a name="Slice"></a><a name="slice">**Slice**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints</dt>
<dd></dd>
<dt><tt>ends</tt> : list of ints (required)</dt>
<dd></dd>
<dt><tt>starts</tt> : list of ints (required)</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>slice</summary>

```python
node = onnx.helper.make_node(
    'Slice',
    inputs=['x'],
    outputs=['y'],
    axes=[0, 1],
    starts=[0, 0],
    ends=[3, 10],
)

x = np.random.randn(20, 10, 5).astype(np.float32)
y = x[0:3, 0:10]

expect(node, inputs=[x], outputs=[y],
       name='test_slice')
```

</details>


<details>
<summary>slice_default_axes</summary>

```python
node = onnx.helper.make_node(
    'Slice',
    inputs=['x'],
    outputs=['y'],
    starts=[0, 0, 3],
    ends=[20, 10, 4],
)

x = np.random.randn(20, 10, 5).astype(np.float32)
y = x[:, :, 3:4]

expect(node, inputs=[x], outputs=[y],
       name='test_slice_default_axes')
```

</details>


<details>
<summary>slice_end_out_of_bounds</summary>

```python
node = onnx.helper.make_node(
    'Slice',
    inputs=['x'],
    outputs=['y'],
    axes=[1],
    starts=[1],
    ends=[1000],
)

x = np.random.randn(20, 10, 5).astype(np.float32)
y = x[:, 1:1000]

expect(node, inputs=[x], outputs=[y],
       name='test_slice_end_out_of_bounds')
```

</details>


<details>
<summary>slice_neg</summary>

```python
node = onnx.helper.make_node(
    'Slice',
    inputs=['x'],
    outputs=['y'],
    axes=[1],
    starts=[0],
    ends=[-1],
)

x = np.random.randn(20, 10, 5).astype(np.float32)
y = x[:, 0:-1]

expect(node, inputs=[x], outputs=[y],
       name='test_slice_neg')
```

</details>


<details>
<summary>slice_start_out_of_bounds</summary>

```python
node = onnx.helper.make_node(
    'Slice',
    inputs=['x'],
    outputs=['y'],
    axes=[1],
    starts=[1000],
    ends=[1000],
)

x = np.random.randn(20, 10, 5).astype(np.float32)
y = x[:, 1000:1000]

expect(node, inputs=[x], outputs=[y],
       name='test_slice_start_out_of_bounds')
```

</details>


### <a name="Softmax"></a><a name="softmax">**Softmax**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>softmax</summary>

```python
node = onnx.helper.make_node(
    'Softmax',
    inputs=['x'],
    outputs=['y'],
)
x = np.array([[-1, 0, 1]]).astype(np.float32)
y = np.exp(x) / np.sum(np.exp(x), axis=1) #expected output [[0.09003058, 0.24472848, 0.66524094]]
expect(node, inputs=[x], outputs=[y],
       name='test_softmax_example')
```

</details>


<details>
<summary>softmax_axis</summary>

```python
def softmax_2d(x):
    max_x = np.max(x, axis=1).reshape((-1, 1))
    exp_x = np.exp(x - max_x)
    return exp_x / np.sum(exp_x, axis=1).reshape((-1, 1))

x = np.array([[0, 1, 2, 3], [10000, 10001, 10002, 10003]]).astype(np.float32)
#expected output [[0.0320586, 0.08714432, 0.23688284, 0.64391428],
#                 [0.0320586, 0.08714432, 0.23688284, 0.64391428]]
y = softmax_2d(x)

node = onnx.helper.make_node(
    'Softmax',
    inputs=['x'],
    outputs=['y'],
)
expect(node, inputs=[x], outputs=[y],
       name='test_softmax_large_number')


x = np.abs(np.random.randn(3, 4, 5).astype(np.float32))
node = onnx.helper.make_node(
    'Softmax',
    inputs=['x'],
    outputs=['y'],
    axis=0,
)
y = softmax_2d(x.reshape(1, 60)).reshape(3, 4, 5)
expect(node, inputs=[x], outputs=[y],
       name='test_softmax_axis_0')

node = onnx.helper.make_node(
    'Softmax',
    inputs=['x'],
    outputs=['y'],
    axis=1,
)
y = softmax_2d(x.reshape(3, 20)).reshape(3, 4, 5)
expect(node, inputs=[x], outputs=[y],
       name='test_softmax_axis_1')

# default axis is 1
node = onnx.helper.make_node(
    'Softmax',
    inputs=['x'],
    outputs=['y'],
)
expect(node, inputs=[x], outputs=[y],
       name='test_softmax_default_axis')

node = onnx.helper.make_node(
    'Softmax',
    inputs=['x'],
    outputs=['y'],
    axis=2,
)
y = softmax_2d(x.reshape(12, 5)).reshape(3, 4, 5)
expect(node, inputs=[x], outputs=[y],
       name='test_softmax_axis_2')
```

</details>


### <a name="Softplus"></a><a name="softplus">**Softplus**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>softplus</summary>

```python
node = onnx.helper.make_node(
    'Softplus',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([-1, 0, 1]).astype(np.float32)
y = np.log(np.exp(x) + 1) #expected output [0.31326166, 0.69314718, 1.31326163]
expect(node, inputs=[x], outputs=[y],
       name='test_softplus_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.log(np.exp(x) + 1)
expect(node, inputs=[x], outputs=[y],
       name='test_softplus')
```

</details>


### <a name="Softsign"></a><a name="softsign">**Softsign**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>softsign</summary>

```python
node = onnx.helper.make_node(
    'Softsign',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([-1, 0, 1]).astype(np.float32)
y = np.array([-0.5, 0, 0.5]).astype(np.float32)
expect(node, inputs=[x], outputs=[y],
       name='test_softsign_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = x / (1 + np.abs(x))
expect(node, inputs=[x], outputs=[y],
       name='test_softsign')
```

</details>


### <a name="SpaceToDepth"></a><a name="spacetodepth">**SpaceToDepth**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>blocksize</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="Split"></a><a name="split">**Split**</a>

#### Versioning

This operator is used if you are using version 2 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 2
}
~~~~

Other versions of this operator: <a href="Changelog.md#Split-1">Split-1</a>

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>split</tt> : list of ints</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs (1 - &#8734;)

<dl>
<dt><tt>outputs</tt> (variadic) : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="Sqrt"></a><a name="sqrt">**Sqrt**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>sqrt</summary>

```python
node = onnx.helper.make_node(
    'Sqrt',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([1, 4, 9]).astype(np.float32)
y = np.sqrt(x) #expected output [1., 2., 3.]
expect(node, inputs=[x], outputs=[y],
       name='test_sqrt_example')

x = np.abs(np.random.randn(3, 4, 5).astype(np.float32))
y = np.sqrt(x)
expect(node, inputs=[x], outputs=[y],
       name='test_sqrt')
```

</details>


### <a name="Squeeze"></a><a name="squeeze">**Squeeze**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints (required)</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>squeezed</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>squeeze</summary>

```python
node = onnx.helper.make_node(
    'Squeeze',
    inputs=['x'],
    outputs=['y'],
    axes=[0],
)
x = np.random.randn(1, 3, 4, 5).astype(np.float32)
y = np.squeeze(x, axis=0)

expect(node, inputs=[x], outputs=[y],
       name='test_squeeze')
```

</details>


### <a name="Sub"></a><a name="sub">**Sub**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>broadcast</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>C</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>sub</summary>

```python
node = onnx.helper.make_node(
    'Sub',
    inputs=['x', 'y'],
    outputs=['z'],
)

x = np.array([1, 2, 3]).astype(np.float32)
y = np.array([3, 2, 1]).astype(np.float32)
z = x - y #expected output [-2., 0., 2.]
expect(node, inputs=[x, y], outputs=[z],
       name='test_sub_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.random.randn(3, 4, 5).astype(np.float32)
z = x - y
expect(node, inputs=[x, y], outputs=[z],
       name='test_sub')
```

</details>


<details>
<summary>sub_broadcast</summary>

```python
node = onnx.helper.make_node(
    'Sub',
    inputs=['x', 'y'],
    outputs=['z'],
    broadcast=1,
)

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.random.randn(5).astype(np.float32)
z = x - y
expect(node, inputs=[x, y], outputs=[z],
       name='test_sub_bcast')
```

</details>


### <a name="Sum"></a><a name="sum">**Sum**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs (1 - &#8734;)

<dl>
<dt><tt>data_0</tt> (variadic) : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>sum</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>sum</summary>

```python
data_0 = np.array([3, 0, 2]).astype(np.float32)
data_1 = np.array([1, 3, 4]).astype(np.float32)
data_2 = np.array([2, 6, 6]).astype(np.float32)
result = np.array([6, 9, 12]).astype(np.float32)
node = onnx.helper.make_node(
    'Sum',
    inputs=['data_0', 'data_1', 'data_2'],
    outputs=['result'],
)
expect(node, inputs=[data_0, data_1, data_2], outputs=[result],
       name='test_sum_example')

node = onnx.helper.make_node(
    'Sum',
    inputs=['data_0'],
    outputs=['result'],
)
expect(node, inputs=[data_0], outputs=[data_0],
       name='test_sum_one_input')

result = np.add(data_0, data_1)
node = onnx.helper.make_node(
    'Sum',
    inputs=['data_0', 'data_1'],
    outputs=['result'],
)
expect(node, inputs=[data_0, data_1], outputs=[result],
       name='test_sum_two_inputs')
```

</details>


### <a name="Tanh"></a><a name="tanh">**Tanh**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>tanh</summary>

```python
node = onnx.helper.make_node(
    'Tanh',
    inputs=['x'],
    outputs=['y'],
)

x = np.array([-1, 0, 1]).astype(np.float32)
y = np.tanh(x) #expected output [-0.76159418, 0., 0.76159418]
expect(node, inputs=[x], outputs=[y],
       name='test_tanh_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.tanh(x)
expect(node, inputs=[x], outputs=[y],
       name='test_tanh')
```

</details>


### <a name="Tile"></a><a name="tile">**Tile**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
<dt><tt>tiles</tt> : T</dt>
<dd></dd>
<dt><tt>axis</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <a name="TopK"></a><a name="topk">**TopK**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>k</tt> : int (required)</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Values</tt> : T</dt>
<dd></dd>
<dt><tt>Indices</tt> : I</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
<dt><tt>I</tt> : tensor(int64), tensor(int32)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>top_k</summary>

```python
node = onnx.helper.make_node(
    'TopK',
    inputs=['x'],
    outputs=['values', 'indices'],
    k=3
)
X = np.array([
    [0, 1, 2, 3],
    [4, 5, 6, 7],
    [8, 9, 10, 11],
], dtype=np.float32)
values_ref = np.array([
    [3, 2, 1],
    [7, 6, 5],
    [11, 10, 9],
])
indices_ref = np.array([
    [3, 2, 1],
    [3, 2, 1],
    [3, 2, 1],
], dtype=np.int32)

expect(node, inputs=[X], outputs=[values_ref, indices_ref],
       name='test_top_k')
```

</details>


### <a name="Transpose"></a><a name="transpose">**Transpose**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>perm</tt> : list of ints</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>transposed</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>all_permutations</summary>

```python
shape = (2,3,4)
data = np.random.random_sample(shape).astype(np.float32)
permutations = list(itertools.permutations(np.arange(len(shape))))

for i in range(len(permutations)):
    node = onnx.helper.make_node(
        'Transpose',
        inputs=['data'],
        outputs=['transposed'],
        perm=permutations[i]
    )            
    transposed = np.transpose(data, permutations[i])
    expect(node, inputs=[data], outputs=[transposed],
        name='test_transpose_all_permutations_' + str(i))            
```

</details>


<details>
<summary>default</summary>

```python
shape = (2, 3, 4)
data = np.random.random_sample(shape).astype(np.float32)

node = onnx.helper.make_node(
    'Transpose',
    inputs=['data'],
    outputs=['transposed']
)

transposed = np.transpose(data)
expect(node, inputs=[data], outputs=[transposed],
    name='test_transpose_default')
```

</details>


### <a name="Unsqueeze"></a><a name="unsqueeze">**Unsqueeze**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axes</tt> : list of ints (required)</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>data</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>expanded</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>squeeze</summary>

```python
node = onnx.helper.make_node(
    'Unsqueeze',
    inputs=['x'],
    outputs=['y'],
    axes=[0],
)
x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.expand_dims(x, axis=0)

expect(node, inputs=[x], outputs=[y],
       name='test_unsqueeze')
```

</details>


### <a name="Xor"></a><a name="xor">**Xor**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>broadcast</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>A</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>C</tt> : T1</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(bool)</dt>
<dd></dd>
<dt><tt>T1</tt> : tensor(bool)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>xor</summary>

```python
node = onnx.helper.make_node(
    'Xor',
    inputs=['x', 'y'],
    outputs=['xor'],
)

# 2d
x = (np.random.randn(3, 4) > 0).astype(np.bool)
y = (np.random.randn(3, 4) > 0).astype(np.bool)
z = np.logical_xor(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_xor2d')

# 3d
x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
y = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
z = np.logical_xor(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_xor3d')

# 4d
x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
y = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
z = np.logical_xor(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_xor4d')
```

</details>


<details>
<summary>xor_axis</summary>

```python
x = (np.random.randn(5, 5, 5, 5) > 0).astype(np.bool)
y = (np.random.randn(5) > 0).astype(np.bool)

node = onnx.helper.make_node(
    'Xor',
    inputs=['x', 'y'],
    outputs=['xor'],
    broadcast=1,
    axis=0,
)

z = np.logical_xor(x, y[:, np.newaxis, np.newaxis, np.newaxis])
expect(node, inputs=[x, y], outputs=[z],
       name='test_xor_axis0')

node = onnx.helper.make_node(
    'Xor',
    inputs=['x', 'y'],
    outputs=['xor'],
    broadcast=1,
    axis=1,
)

z = np.logical_xor(x, y[:, np.newaxis, np.newaxis,])
expect(node, inputs=[x, y], outputs=[z],
       name='test_xor_axis1')

node = onnx.helper.make_node(
    'Xor',
    inputs=['x', 'y'],
    outputs=['xor'],
    broadcast=1,
    axis=2,
)

z = np.logical_xor(x, y[:, np.newaxis,])
expect(node, inputs=[x, y], outputs=[z],
       name='test_xor_axis2')

node = onnx.helper.make_node(
    'Xor',
    inputs=['x', 'y'],
    outputs=['xor'],
    broadcast=1,
    axis=3,
)

z = np.logical_xor(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_xor_axis3')
```

</details>


<details>
<summary>xor_broadcast</summary>

```python
node = onnx.helper.make_node(
    'Xor',
    inputs=['x', 'y'],
    outputs=['xor'],
    broadcast=1,
)

#3d vs 1d
x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
y = (np.random.randn(5) > 0).astype(np.bool)
z = np.logical_xor(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_xor_bcast3v1d')

#3d vs 2d
x = (np.random.randn(3, 4, 5) > 0).astype(np.bool)
y = (np.random.randn(4, 5) > 0).astype(np.bool)
z = np.logical_xor(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_xor_bcast3v2d')

#4d vs 2d
x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
y = (np.random.randn(5, 6) > 0).astype(np.bool)
z = np.logical_xor(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_xor_bcast4v2d')

#4d vs 3d
x = (np.random.randn(3, 4, 5, 6) > 0).astype(np.bool)
y = (np.random.randn(4, 5, 6) > 0).astype(np.bool)
z = np.logical_xor(x, y)
expect(node, inputs=[x, y], outputs=[z],
       name='test_xor_bcast4v3d')
```

</details>


### <sub>experimental</sub> <a name="ATen"></a><a name="aten">**ATen**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs (1 - &#8734;)

<dl>
<dt><tt>input</tt> (variadic) : T</dt>
<dd></dd>
</dl>

#### Outputs (1 - &#8734;)

<dl>
<dt><tt>output</tt> (variadic) : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(bool), tensor(int32), tensor(int64), tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="Affine"></a><a name="affine">**Affine**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>alpha</tt> : float</dt>
<dd></dd>
<dt><tt>beta</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="ConstantFill"></a><a name="constantfill">**ConstantFill**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>dtype</tt> : int</dt>
<dd></dd>
<dt><tt>extra_shape</tt> : list of ints</dt>
<dd></dd>
<dt><tt>input_as_shape</tt> : int</dt>
<dd></dd>
<dt><tt>shape</tt> : list of ints</dt>
<dd></dd>
<dt><tt>value</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs (0 - 1)

<dl>
<dt><tt>input</tt> (optional) : T1</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T2</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T1</tt> : tensor(float), tensor(int32), tensor(int64), tensor(bool)</dt>
<dd></dd>
<dt><tt>T2</tt> : tensor(float), tensor(int32), tensor(int64), tensor(bool)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="Crop"></a><a name="crop">**Crop**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>border</tt> : list of ints</dt>
<dd></dd>
<dt><tt>scale</tt> : list of ints</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="FC"></a><a name="fc">**FC**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
<dt><tt>axis_w</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
<dt><tt>W</tt> : T</dt>
<dd></dd>
<dt><tt>B</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="GRUUnit"></a><a name="gruunit">**GRUUnit**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>drop_states</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>hidden_prev</tt> : T</dt>
<dd></dd>
<dt><tt>gates</tt> : T</dt>
<dd></dd>
<dt><tt>seq_lengths</tt> : T</dt>
<dd></dd>
<dt><tt>t</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>hidden</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="GivenTensorFill"></a><a name="giventensorfill">**GivenTensorFill**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>extra_shape</tt> : list of ints</dt>
<dd></dd>
<dt><tt>input_as_shape</tt> : int</dt>
<dd></dd>
<dt><tt>shape</tt> : list of ints</dt>
<dd></dd>
<dt><tt>values</tt> : list of floats</dt>
<dd></dd>
</dl>

#### Inputs (0 - 1)

<dl>
<dt><tt>shape</tt> (optional) : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="Identity"></a><a name="identity">**Identity**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="If"></a><a name="if">**If**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>else_branch</tt> : graph (required)</dt>
<dd></dd>
<dt><tt>then_branch</tt> : graph (required)</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>cond</tt> : B</dt>
<dd></dd>
</dl>

#### Outputs (1 - &#8734;)

<dl>
<dt><tt>outputs</tt> (variadic) : V</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>V</tt> : tensor(float), tensor(int32), tensor(string), tensor(bool), tensor(uint8), tensor(int8), tensor(uint16), tensor(int16), tensor(int64), tensor(float16), tensor(double)</dt>
<dd></dd>
<dt><tt>B</tt> : tensor(bool)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="ImageScaler"></a><a name="imagescaler">**ImageScaler**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>bias</tt> : list of floats</dt>
<dd></dd>
<dt><tt>scale</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="Loop"></a><a name="loop">**Loop**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>body</tt> : graph (required)</dt>
<dd></dd>
</dl>

#### Inputs (3 - &#8734;)

<dl>
<dt><tt>M</tt> : I</dt>
<dd></dd>
<dt><tt>cond</tt> : B</dt>
<dd></dd>
<dt><tt>v_initial</tt> (variadic) : V</dt>
<dd></dd>
</dl>

#### Outputs (1 - &#8734;)

<dl>
<dt><tt>v_final_and_scan_outputs</tt> (variadic) : V</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>V</tt> : tensor(float), tensor(int32), tensor(string), tensor(bool), tensor(uint8), tensor(int8), tensor(uint16), tensor(int16), tensor(int64), tensor(float16), tensor(double)</dt>
<dd></dd>
<dt><tt>I</tt> : int64</dt>
<dd></dd>
<dt><tt>B</tt> : bool</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="LoopIndexTensor"></a><a name="loopindextensor">**LoopIndexTensor**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>axis</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>T</tt> : T</dt>
<dd></dd>
<dt><tt>loop_idx</tt> : I</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>O</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float), tensor(int32), tensor(string), tensor(bool), tensor(uint8), tensor(int8), tensor(uint16), tensor(int16), tensor(int64), tensor(float16), tensor(double)</dt>
<dd></dd>
<dt><tt>I</tt> : int32</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="MeanVarianceNormalization"></a><a name="meanvariancenormalization">**MeanVarianceNormalization**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>across_channels</tt> : int</dt>
<dd></dd>
<dt><tt>normalize_variance</tt> : int</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="ParametricSoftplus"></a><a name="parametricsoftplus">**ParametricSoftplus**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>alpha</tt> : float</dt>
<dd></dd>
<dt><tt>beta</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="Scale"></a><a name="scale">**Scale**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>scale</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="ScaledTanh"></a><a name="scaledtanh">**ScaledTanh**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>alpha</tt> : float</dt>
<dd></dd>
<dt><tt>beta</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>input</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>output</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


### <sub>experimental</sub> <a name="ThresholdedRelu"></a><a name="thresholdedrelu">**ThresholdedRelu**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>alpha</tt> : float</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


#### Examples

<details>
<summary>default</summary>

```python
default_alpha = 1.0
node = onnx.helper.make_node(
    'ThresholdedRelu',
    inputs=['x'],
    outputs=['y']
)
x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x, default_alpha, np.inf)
y[y == default_alpha] = 0

expect(node, inputs=[x], outputs=[y],
       name='test_thresholdedrelu_default')
```

</details>


<details>
<summary>thresholdedrelu</summary>

```python
alpha = 2.0
node = onnx.helper.make_node(
    'ThresholdedRelu',
    inputs=['x'],
    outputs=['y'],
    alpha=alpha
)

x = np.array([-1.5, 0., 1.2, 2.0, 2.2]).astype(np.float32)
y = np.clip(x, alpha, np.inf)  # expected output [0., 0., 0., 0., 2.2]
y[y == alpha] = 0

expect(node, inputs=[x], outputs=[y],
       name='test_thresholdedrelu_example')

x = np.random.randn(3, 4, 5).astype(np.float32)
y = np.clip(x, alpha, np.inf)
y[y == alpha] = 0

expect(node, inputs=[x], outputs=[y],
       name='test_thresholdedrelu')
```

</details>


### <sub>experimental</sub> <a name="Upsample"></a><a name="upsample">**Upsample**</a>

#### Versioning

This operator is used if you are using version 1 of the default ONNX operator set until the next BC-breaking change to this operator; e.g., it will be used if your protobuf has:

~~~~
opset_import {
  version = 1
}
~~~~

#### Attributes

<dl>
<dt><tt>height_scale</tt> : float (required)</dt>
<dd></dd>
<dt><tt>mode</tt> : string</dt>
<dd></dd>
<dt><tt>width_scale</tt> : float (required)</dt>
<dd></dd>
</dl>

#### Inputs

<dl>
<dt><tt>X</tt> : T</dt>
<dd></dd>
</dl>

#### Outputs

<dl>
<dt><tt>Y</tt> : T</dt>
<dd></dd>
</dl>

#### Type Constraints

<dl>
<dt><tt>T</tt> : tensor(bool), tensor(int32), tensor(int64), tensor(float16), tensor(float), tensor(double)</dt>
<dd></dd>
</dl>


